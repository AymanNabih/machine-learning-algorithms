{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this ipython notebook I will implement Linear Regression and its various variations and optimized variations from scratch. To understand the implementation I will also plot the optimizations process in cases of Gradient descent. In this notebook we will implement each algorithm with its own class and then compare performance with sklearn's implementation of algorithm. <br>\n",
    "To use and take advantage of sklearn's consisten and easy to use interface, I will creating our custom implementation using sklearn's base classes **BaseEstimator**, **RegressionMixin**.\n",
    "<br>\n",
    "### Linear Regression (Ordinary Least Squares)\n",
    "* Simple Linear Regression \n",
    "* Multiple Linear Regression\n",
    "<br>\n",
    "\n",
    "### Regularized Variations\n",
    "To prevent overfitting in polynomial regression I will also implement with Regularization\n",
    "* Lasso (L1 Norm)\n",
    "* Ridge (L2 Norm)\n",
    "* Elastic Net (Linear Combination of L1 and L2 norm - L1 + L2)\n",
    "\n",
    "### Gradient Descent for model parameters\n",
    "I will also implement Linear Regression with Gradient Descent Convex Optimization\n",
    "* Batch Gradient Descent\n",
    "* Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "dataset = pd.read_csv('data.csv')\n",
    "X = dataset.iloc[:,0].values.reshape(-1,1)\n",
    "y = dataset.iloc[:,1].values\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X,y)\n",
    "\n",
    "xScaler = StandardScaler()\n",
    "xTrain = xScaler.fit_transform(xTrain)\n",
    "xTest = xScaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "First let's see at simple linear regression and their mathematical formulas. All the formulas presented in this or other notebooks won't be dervied, that's out of scope for these notebooks. For more explanation read full post at my blog [sahilsehwag](http://www.sahilsehwag.wordpress.com)\n",
    "<br>\n",
    "\n",
    "## $covariance(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}$\n",
    "## $variance(x) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}$\n",
    "Note that we are using \"m\" but in most of the books or any kind of resources you will find this as referred to as $\\beta$, don't get confused we are just using \"m\" for familiarity\n",
    "### $y = mx + b$\n",
    "## $m=\\frac{covariance(x,y)}{variance(x)}$\n",
    "### $b=\\bar{y} - m\\bar{x} $\n",
    "\n",
    "Let's look at our cost function. The cost function we are using is called *Residual Sum of Squares*\n",
    "### $R_{SS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) = \\sum_{i=1}^{n} (y_i - mx_i)$\n",
    "\n",
    "Finally We will look at our scoring function which is called *Coefficeint of Determination*\n",
    "## $r^2 = 1 - \\frac{R_{ss}(\\hat{y})}{R_{ss}(\\bar{y})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, plot=False):\n",
    "        self.plot = plot\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._simple_linear_regression(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.m * x[0] + self.c\n",
    "                ys.append(y)\n",
    "            return ys\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = ((y - y_pred) ** 2).sum()\n",
    "            rss_mean = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _simple_linear_regression(self, X, y=None):\n",
    "        cov = ((X[:,0] - X[:,0].mean()) * (y - y.mean())).sum() / (len(y)-1)\n",
    "        var = ((X[:,0] - X[:,0].mean()) ** 2).sum() / (len(y) - 1)\n",
    "        self.m = cov/var\n",
    "        self.c = y.mean() - self.m * X[:,0].mean()\n",
    "        return\n",
    "    \n",
    "    def _check_params(self, X, y):\n",
    "        if type(self.plot) != bool:\n",
    "            raise Exception('\"plot\" should be boolean')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression vs CustomLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02554728  0.4368973   0.20923646]\n",
      "[ 0.02554728  0.4368973   0.20923646]\n"
     ]
    }
   ],
   "source": [
    "skModel = LinearRegression()\n",
    "custModel = CustomLinearRegression()\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)\n",
    "\n",
    "print(cross_val_score(estimator=skModel, X=xTest, y=yTest, cv=3))\n",
    "print(cross_val_score(estimator=custModel, X=xTest, y=yTest, cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "Multiple Linear Regression is just a generalization of Simple Linear Regression. It is a generalization for n dimensions.\n",
    "\n",
    "## $ y = b + \\sum_{i=1}^{n} m_ix_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultipleLinearRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._multiple_linear_regression(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.b + (self.M * x).sum()\n",
    "                ys.append(y)\n",
    "            return ys\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = ((y - y_pred) ** 2).sum()\n",
    "            rss_mean = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _multiple_linear_regression(self, X, y=None):\n",
    "        M = np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.dot(np.transpose(X), y))\n",
    "        \n",
    "        b = yTest.mean()\n",
    "        for i in range(X.shape[1]):\n",
    "            b -= M[i] * X[:,i].mean()\n",
    "        \n",
    "        self.M = M\n",
    "        self.b = b\n",
    "        return\n",
    "        \n",
    "            \n",
    "    def _check_params(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset with multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = boston\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression vs MultipleLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75293615  0.59266436  0.53107781]\n",
      "[ 0.74489941  0.54326411  0.43325086]\n"
     ]
    }
   ],
   "source": [
    "skModel = LinearRegression()\n",
    "custModel = MultipleLinearRegression()\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)\n",
    "\n",
    "print(cross_val_score(estimator=skModel, X=xTest, y=yTest, cv=3))\n",
    "print(cross_val_score(estimator=custModel, X=xTest, y=yTest, cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "When doing polynomial regression, higher degree terms can lead to overfitting. To solve this problem regularization is used which adds a penalty term that increases penalty as we add more higher degree terms.<br>\n",
    "Mainly we do this by adding penalty to our **Residual Sum of Squares**. There are two regularization techniques which are:\n",
    "* L1 Norm i.e Lasso\n",
    "* L2 Norm i.e Ridge\n",
    "* ElasticNet uses Linear Combination of L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "\n",
    "In previous section we mentioned Lasso uses L1 regularization. Lasso stands for **_Least Absolute Shrinkage and Selection Operator_**. <br>\n",
    "$\\lambda$ is called **_Shrinkage Parameter_**. It is a hyperparameter that we have to select ourselves\n",
    "\n",
    "## $ R_{ss} = (\\sum_{i=1}^{n} y_i - mx^p_i) + \\lambda \\sum_{j=1}^{p} m_j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomLasso(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=1.0, tol=0.0001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._multiple_linear_regression(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.b + (self.M * x).sum()\n",
    "                ys.append(y)\n",
    "            return ys\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) for m in self.M])\n",
    "            rss_mean = ((y - y.mean()) ** 2).sum() + np.sum([(m * self.alpha) for m in self.M])\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _multiple_linear_regression(self, X, y=None):\n",
    "        M = np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.dot(np.transpose(X), y))\n",
    "        \n",
    "        b = yTest.mean()\n",
    "        for i in range(X.shape[1]):\n",
    "            b -= M[i] * X[:,i].mean()\n",
    "        \n",
    "        self.M = M\n",
    "        self.b = b\n",
    "        return\n",
    "        \n",
    "            \n",
    "    def _check_params(self, X, y):\n",
    "        if type(self.alpha) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"alpha\"')\n",
    "            \n",
    "        if type(self.tol) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"tol\"')\n",
    "        \n",
    "        if type(self.max_iter) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"max_iter\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso vs CustomLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54637001  0.79931518  0.33287788  0.74097513  0.50645879]\n",
      "[ 0.69778664  0.80636858  0.42816501  0.77958305  0.14928785]\n"
     ]
    }
   ],
   "source": [
    "skModel = Lasso()\n",
    "custModel = CustomLasso()\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)\n",
    "\n",
    "print(cross_val_score(estimator=skModel, X=xTest, y=yTest, cv=5))\n",
    "print(cross_val_score(estimator=custModel, X=xTest, y=yTest, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge\n",
    "\n",
    "In previous section we mentioned Ridge uses L2 regularization\n",
    "\n",
    "## $ R_{ss} = (\\sum_{i=1}^{n} y_i - mx^p_i) + \\lambda \\sum_{j=1}^{p} m_j^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomRidge(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=1.0, tol=0.0001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._multiple_linear_regression(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.b + (self.M * x).sum()\n",
    "                ys.append(y)\n",
    "            return ys\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "            rss_mean = ((y - y.mean()) ** 2).sum() + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _multiple_linear_regression(self, X, y=None):\n",
    "        M = np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.dot(np.transpose(X), y))\n",
    "        \n",
    "        b = yTest.mean()\n",
    "        for i in range(X.shape[1]):\n",
    "            b -= M[i] * X[:,i].mean()\n",
    "        \n",
    "        self.M = M\n",
    "        self.b = b\n",
    "        return\n",
    "        \n",
    "            \n",
    "    def _check_params(self, X, y):\n",
    "        if type(self.alpha) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"alpha\"')\n",
    "            \n",
    "        if type(self.tol) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"tol\"')\n",
    "        \n",
    "        if type(self.max_iter) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"max_iter\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ridge vs CustomRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65866101  0.81956924  0.4712709   0.65542133  0.37941599]\n",
      "[ 0.61016662  0.76204059  0.4082369   0.68379681  0.14352686]\n"
     ]
    }
   ],
   "source": [
    "skModel = Ridge()\n",
    "custModel = CustomRidge()\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)\n",
    "\n",
    "print(cross_val_score(estimator=skModel, X=xTest, y=yTest, cv=5))\n",
    "print(cross_val_score(estimator=custModel, X=xTest, y=yTest, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ElasticNet\n",
    "## $ R_{ss} = (\\sum_{i=1}^{n} y_i - mx^p_i) + \\lambda \\sum_{j=1}^{p} m_j+ \\lambda \\sum_{j=1}^{p} m_j^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomElasticNet(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=1.0, tol=0.0001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._multiple_linear_regression(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.b + (self.M * x).sum()\n",
    "                ys.append(y)\n",
    "            return ys\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) for m in self.M]) + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "            rss_mean = ((y - y.mean()) ** 2).sum() + np.sum([(m * self.alpha) ** 2 for m in self.M]) + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _multiple_linear_regression(self, X, y=None):\n",
    "        M = np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.dot(np.transpose(X), y))\n",
    "        \n",
    "        b = yTest.mean()\n",
    "        for i in range(X.shape[1]):\n",
    "            b -= M[i] * X[:,i].mean()\n",
    "        \n",
    "        self.M = M\n",
    "        self.b = b\n",
    "        return\n",
    "        \n",
    "            \n",
    "    def _check_params(self, X, y):\n",
    "        if type(self.alpha) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"alpha\"')\n",
    "            \n",
    "        if type(self.tol) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"tol\"')\n",
    "        \n",
    "        if type(self.max_iter) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"max_iter\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet vs CustomElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57552499  0.80697554  0.37256972  0.7590873   0.51621722]\n",
      "[ 0.6468721   0.76899432  0.43179247  0.71058854  0.17398641]\n"
     ]
    }
   ],
   "source": [
    "skModel = ElasticNet()\n",
    "custModel = CustomElasticNet()\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)\n",
    "\n",
    "print(cross_val_score(estimator=skModel, X=xTest, y=yTest, cv=5))\n",
    "print(cross_val_score(estimator=custModel, X=xTest, y=yTest, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomSGDRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=0.0001, tol=0.0001, max_iter=1000, penalty='l2'):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.penalty = penalty\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._check_params(X, y)\n",
    "        self._sgd(X, y)\n",
    "        self.fitted_ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"predict()\" called before fit()')\n",
    "        else:\n",
    "            ys = list()\n",
    "            for x in X:\n",
    "                y = self.b + (self.M * x).sum()\n",
    "                ys.append(y)\n",
    "            return np.array(ys)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        if self.fitted_ == None:\n",
    "            raise Exception('\"score()\" called before fit()')\n",
    "        else:\n",
    "            y_pred = self.predict(X)\n",
    "            rss = self._cost(X,y,y_pred)\n",
    "            rss_mean = self._cost(X,y,y.mean())\n",
    "            return 1 - (rss/rss_mean)\n",
    "    \n",
    "    def _sgd(self, X, y=None):\n",
    "        m = np.array([0 for _ in range(X.shape[1])], dtype=np.float64)\n",
    "        b = 0\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            m_slope = np.array([0 for _ in range(X.shape[1])], dtype=np.float64)\n",
    "            b_slope = 0\n",
    "            for j in range(y.shape[0]):\n",
    "                xi = X[j]\n",
    "                yi = y[j]\n",
    "                \n",
    "                m_slope += (-2/y.shape[0]) * (yi - (m * xi).sum() - b) * xi\n",
    "                b_slope += (-2/y.shape[0]) * (yi - (m * xi).sum() - b)\n",
    "            \n",
    "            m = m - m_slope * self.alpha\n",
    "            b = b - b_slope * self.alpha\n",
    "            \n",
    "        self.M = m\n",
    "        self.b = b\n",
    "        return\n",
    "        \n",
    "    def _cost(self, X, y, y_pred):\n",
    "        if self.penalty == None:\n",
    "            rss = ((y - y_pred) ** 2).sum()\n",
    "        elif self.penalty == 'l1':\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) for m in self.M])\n",
    "        elif self.penalty == 'l2':\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "        elif self.penalty == 'elasticnet':\n",
    "            rss = ((y - y_pred) ** 2).sum() + np.sum([(m * self.alpha) for m in self.M]) + np.sum([(m * self.alpha) ** 2 for m in self.M])\n",
    "        \n",
    "        return rss\n",
    "            \n",
    "    def _check_params(self, X, y):\n",
    "        if type(self.alpha) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"alpha\"')\n",
    "            \n",
    "        if type(self.tol) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"tol\"')\n",
    "        \n",
    "        if type(self.max_iter) in (float, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"max_iter\"')\n",
    "            \n",
    "        if type(self.penalty) in (str,None) and self.penalty in ('l1', 'l2', 'elasticnet'):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Invalid type of \"penalty\"')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = boston\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X,y)\n",
    "\n",
    "xScaler = StandardScaler()\n",
    "xTrain = xScaler.fit_transform(xTrain)\n",
    "xTest = xScaler.transform(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor vs CustomSGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomSGDRegressor(alpha=0.01, max_iter=1000, penalty='l2', tol=0.0001)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skModel = SGDRegressor(tol=0.0001)\n",
    "custModel = CustomSGDRegressor(alpha=0.01)\n",
    "\n",
    "skModel.fit(xTrain, yTrain)\n",
    "custModel.fit(xTrain, yTrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "498px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
